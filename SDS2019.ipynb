{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Data Science Exam 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exam number: 202, 177, 43, 189\n",
    "\n",
    "This is a project is about the housing market in Copenhagen focusing on the relationship between location of metro stations and the housing prices. The following code contains following elements:\n",
    "- Scraping data and parsing location data\n",
    "    - From Boliga \n",
    "    - For metro and S-train stations \n",
    "- Analysis \n",
    "    - Creating variables\n",
    "    - Data validition\n",
    "    - Statitical analysis\n",
    "- Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing relevant modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium import plugins\n",
    "import folium\n",
    "from selenium import webdriver\n",
    "import time, os, glob, re\n",
    "import pandas as pd\n",
    "from Connector import Connector, ratelimit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import random\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.dates as mdates\n",
    "from geopy.distance import geodesic\n",
    "from dateutil.parser import parse\n",
    "from geopy.geocoders import Nominatim\n",
    "import statsmodels\n",
    "from tabulate import tabulate\n",
    "from skmisc.loess import loess\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping data and parsing location\n",
    "Scraping housing data from Boliga.dk and the station data from Wikipeadia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping data from Boliga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of selenium\n",
    "path2gecko = '/Users/MacbookJos/git/geckodriver' # define path geckodriver\n",
    "browser = webdriver.Firefox(executable_path=path2gecko)\n",
    "\n",
    "# Direction to boliga.dk\n",
    "browser.get('https://www.boliga.dk/salg/resultater?salesDateMin=1995&zipcodeFrom=1000&zipcodeTo=2499&searchTab=1&page=1&sort=date-a')\n",
    "cookie_button = browser.find_element_by_xpath('//*[@id=\"coiAccept\"]')\n",
    "cookie_button.click()\n",
    "\n",
    "# Defines function that process the data\n",
    "def df2table(html):\n",
    "    df = pd.read_html(html)[0]\n",
    "    df = df.iloc[:, 0:8]\n",
    "    \n",
    "    # Translating and renaming columns\n",
    "    for column in list(df.columns):\n",
    "        df[column] = df[column].str.replace(column, '')\n",
    "    df.columns = [\"Address\", \"Sell_price\", \"Date_of_sale\", \"Type\",\n",
    "                  \"sqm_price\", \"Rooms\", \"m2\", \"Building_year\"]\n",
    "\n",
    "    # Removing dublicates of the type of residence\n",
    "    for i in range(len(df['Type'])):\n",
    "        for j in ['EEjerlejlighed', 'VVilla', 'RRækkehus', 'FFritidshus']:\n",
    "            df['Type'][i] = df['Type'][i].replace(j,'')\n",
    "\n",
    "    # Changing the prices, so that it's numeric values\n",
    "    for s in range(len(df['Sell_price'])):\n",
    "        df['Sell_price'][s] = df['Sell_price'][s].replace('.','')\n",
    "        df['sqm_price'][s] = df['sqm_price'][s].replace('.','')\n",
    "    \n",
    "    df['sqm_price'] = pd.to_numeric(df['sqm_price'])\n",
    "    df['Sell_price'] = pd.to_numeric(df['Sell_price'])\n",
    "    \n",
    "    #Changing the date into date-time format\n",
    "    def format_dates(date):\n",
    "        q = date.split('-')\n",
    "        return q[0].strip() + q[1].strip() + q[2]\n",
    "\n",
    "    df['Date_of_sale'] = df['Date_of_sale'].apply(lambda x: format_dates(x))\n",
    "    df['Date_of_sale'] = pd.to_datetime(df['Date_of_sale'], format='%d%m%Y')\n",
    "\n",
    "    return df\n",
    "\n",
    "i = 1\n",
    "dir = str(os.getcwd())\n",
    "while i <= 288: #288\n",
    "    ## Initialization of log\n",
    "    logfile = 'log_boliga_csv'\n",
    "    project_name = 'SDS exam'\n",
    "    header = ['id','project','connector_type','t', 'delta_t', 'url',\\\n",
    "              'redirect_url','response_size', 'response_code','success', 'error']\n",
    "\n",
    "    if os.path.isfile(logfile):\n",
    "        log = open(logfile,'a')\n",
    "    else:\n",
    "        log = open(logfile,'w')\n",
    "        log.write(';'.join(header))\n",
    "\n",
    "    ## load log\n",
    "    with open(logfile,'r') as f: # open file\n",
    "        l = f.read().split('\\n') # read and split file by newlines.\n",
    "        ## set id\n",
    "        if len(l)<=1:\n",
    "            id = 0\n",
    "        else:\n",
    "            id = int(l[-1][0])+1\n",
    "\n",
    "    t = time.time()\n",
    "\n",
    "    #Creating the log file\n",
    "    \"\"\"\n",
    "    NOTE: several indicators are different than from the indicators\n",
    "    that are established with the requests package. `dt`, does not\n",
    "    necessarily reflect the complete load time. `size` might not be\n",
    "    correct as selenium works in the background and could still be\n",
    "    loading\n",
    "    \"\"\"\n",
    "    html = browser.page_source\n",
    "    df = df2table(html)\n",
    "\n",
    "    ## Key arguments\n",
    "    err = ''                           # define python error variable as empty assumming success.\n",
    "    success = True                     # define success variable\n",
    "    connector_type = \"selenium\"\n",
    "    redirect_url = browser.current_url # log current url, after potential redirects\n",
    "    dt = t - time.time()               # define delta-time waiting for the server and downloading content.\n",
    "    size = df.memory_usage(deep=True, index=True).sum()    # define variable for size of html content of the response.\n",
    "    response_code = ''                 # log status code.\n",
    "\n",
    "    ## log...\n",
    "    call_id = id                        # get current unique identifier for the call\n",
    "\n",
    "    # Defines row to be written in the log\n",
    "    row = [call_id, project_name, connector_type, t, dt,\\\n",
    "           redirect_url, size, response_code, success, err] # define row\n",
    "\n",
    "    log.write('\\n'+';'.join(map(str,row))) # write row to log file.\n",
    "    log.flush()\n",
    "\n",
    "    # storing parsed table as csv\n",
    "    file_path = str(dir + '/boliga' +'/scrape' +'/%d.csv'%i)\n",
    "    with open(file_path, mode='w', encoding='UTF-8',\n",
    "              errors='strict', buffering=1) as f:\n",
    "        f.write(df.to_csv())\n",
    "    time.sleep(2)\n",
    "\n",
    "    xpath_next = '/html/body/app-root/app-sold-properties-list/div[2]/app-pagination/div[1]/div[4]/a'\n",
    "    next_button = browser.find_element_by_xpath(xpath_next)\n",
    "    next_button.click()\n",
    "    time.sleep(1)\n",
    "    i += 1\n",
    "\n",
    "########################    \n",
    "# Apppending csv-files #\n",
    "########################\n",
    "\n",
    "#Path to file directory\n",
    "boliga_directory = dir + '/boliga/scrape/'\n",
    "\n",
    "#import all the files in the folder\n",
    "boliga_files = glob.glob(os.path.join(boliga_directory, '*.csv'))\n",
    "\n",
    "#loop throug all files and read as pandas\n",
    "merged_df = [] #saves as list of dataframes\n",
    "for boliga_file in boliga_files:\n",
    "    df = pd.read_csv(boliga_file)\n",
    "    merged_df.append(df)\n",
    "\n",
    "#merge the dataframes with concat\n",
    "boliga_data = pd.concat(merged_df, ignore_index=True)\n",
    "\n",
    "#save as csv\n",
    "file_path = str(dir + '/boliga/data/housing_data.csv')\n",
    "with open(file_path, mode='w', encoding='UTF-8',\n",
    "          errors='strict', buffering=1) as f:\n",
    "    f.write(boliga_data.to_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing location data onto Boliga data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calling the geolocator from geopy\n",
    "geolocator = Nominatim(user_agent=\"SDS Student\")\n",
    "geocode = RateLimiter(geolocator.geocode, min_delay_seconds=1.1,\n",
    "                       max_retries=3, error_wait_seconds=5,\n",
    "                       return_value_on_exception=True)\n",
    "\n",
    "## Defining a function which downloads information about longtitude and latitude\n",
    "def address_parser (address_list):\n",
    "    street =  []\n",
    "    locations = []\n",
    "    latitude = []\n",
    "    longitude = []\n",
    "    for addr in address_list:\n",
    "        loc = geocode(addr)\n",
    "        time.sleep(.2)\n",
    "        if loc != None:\n",
    "            lati = loc.latitude\n",
    "            time.sleep(.2)\n",
    "            long = loc.longitude\n",
    "            time.sleep(.2)\n",
    "            stre = loc.address\n",
    "            time.sleep(.2)\n",
    "        else:\n",
    "            stre, lati, long = None, None, None\n",
    "        latitude.append(lati)\n",
    "        longitude.append(long)\n",
    "        locations.append(stre)\n",
    "    return address_list, locations, latitude, longitude\n",
    "\n",
    "dir = str(os.getcwd())\n",
    "datafile = dir + \"/boliga/data/housing_data.csv\"\n",
    "boliga_data = pd.read_csv(datafile)\n",
    "\n",
    "## cleaning DataFrame\n",
    "boliga_data['Date_of_sale'] = pd.to_datetime(boliga_data['Date_of_sale'])\n",
    "boliga_data.sort_values(by=['Date_of_sale'], inplace=True, ascending=True)\n",
    "boliga_data = boliga_data.reset_index()\n",
    "boliga_data = boliga_data.iloc[:, 3:]\n",
    "df_address = pd.DataFrame([a[0] + ' ' + a[-1][6:]\\\n",
    "                           for a in boliga_data['Address'].str.split(',')])\n",
    "df_address.drop_duplicates(inplace = True)\n",
    "df_address[0] = df_address[0].str.lstrip()\n",
    "\n",
    " #check if folder exists\n",
    "project = dir + '/boliga/location'\n",
    "if not os.path.isdir(project):\n",
    "     os.mkdir(project)\n",
    "\n",
    "print(time.time())\n",
    " chunksize = 100\n",
    "dt = []\n",
    "for i, chunk in df_address.groupby(np.arange(len(df_address)) // chunksize):\n",
    "    t = time.time()\n",
    "    building_address, locations, latitude, longitude = address_parser(chunk[0])\n",
    "    dt.append(t - time.time())\n",
    "    dict = {'building_address':building_address,'location':locations, 'latitude':latitude, 'longitude': longitude}\n",
    "    df = pd.DataFrame(dict)\n",
    "    file_path = dir + \"/boliga/location/location_data_\" + str(i) + \".csv\"\n",
    "    with open(file_path, mode='w', encoding='UTF-8',\n",
    "                   errors='strict', buffering=1) as f:\n",
    "        f.write(df.to_csv())\n",
    "print(time.time())\n",
    "\n",
    "file_path = dir + \"/boliga/location/location_data_log\"\n",
    "with open(file_path, mode='w', encoding='UTF-8',\n",
    "          errors='strict', buffering=1) as f:\n",
    "    f.write(dt)\n",
    "    \n",
    "#Transforming into a pandas dataframe\n",
    "#Path to file directory\n",
    "location_dir = dir + '/boliga/location/'\n",
    "\n",
    "#import all the files in the folder\n",
    "location_files = glob.glob(os.path.join(location_dir, '*.csv'))\n",
    "\n",
    "#loop throug all files and read as pandas\n",
    "merged_df = [] #saves as list of dataframes\n",
    "for file in location_files:\n",
    "    df = pd.read_csv(file)\n",
    "    merged_df.append(df)\n",
    "\n",
    " #merge the dataframes with concat\n",
    " location_data = pd.concat(merged_df, ignore_index=True)\n",
    "\n",
    "#save as csv\n",
    "file_path = str(dir + '/boliga/data/location_data.csv')\n",
    "with open(file_path, mode='w', encoding='UTF-8',\n",
    "          errors='strict', buffering=1) as f:\n",
    "    f.write(location_data.to_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping station data and parsing location data onto them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Metro stations #\n",
    "##################\n",
    "\n",
    "#Logfile of metro stations\n",
    "logfile = 'log_station_scrape.csv'## name your log file.\n",
    "connector = Connector(logfile)\n",
    "\n",
    "# Scrape initial table of stations\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_Copenhagen_Metro_stations\"\n",
    "resp, callid = connector.get(url, 'mstation_scrape')\n",
    "html = resp.text\n",
    "\n",
    "# parse table\n",
    "table = pd.read_html(resp.text)[1]\n",
    "table = table.drop(['Transfer', 'Line'], axis = 1)\n",
    "table['Station'] = table['Station']\n",
    "\n",
    "# Importing meta-data on stations\n",
    "def links (html):\n",
    "    s = BeautifulSoup(html, 'html.parser')\n",
    "    table_html = s.find_all('table')[1]\n",
    "    urlFmt= re.compile('href=\"(\\S*)\"')\n",
    "    link_locations = urlFmt.findall(str(table_html))\n",
    "    links = ['https://en.wikipedia.org' + i for i in link_locations]\n",
    "    links = [link for link in links if '/wiki/' in link]\n",
    "    for i in ['S-train', 'M3_', 'M4_', 'M2_', 'M1_', 'S-train', 'Template',\n",
    "              'K%C3%B8ge_Nord','File:', 'List', 'commons.', '_Metro', '_Line',\n",
    "              'Airport', 'Lokaltog', 'Letbane']:\n",
    "        links = [link for link in links if not i in link]\n",
    "    return sorted(list(set(links)))\n",
    "\n",
    "# Function whih gets lontitutes and latitudes of stations\n",
    "def mstation_location(urls):\n",
    "    stations = []\n",
    "    longitudes = []\n",
    "    latitudes = []\n",
    "    for station in urls:\n",
    "        html = requests.get(station).text\n",
    "        s = BeautifulSoup(html, 'lxml')\n",
    "        loc = s.select('span.geo-dms')[0]\n",
    "        latFmt = re.compile('.*latitude\">(.*)</span> <')\n",
    "        lonFmt = re.compile('.*longitude\">(.*)</span><')\n",
    "        longitudes.append(lonFmt.findall(str(loc)))\n",
    "        latitudes.append(latFmt.findall(str(loc)))\n",
    "        stations.append(str(s.title.string).replace(' Station - Wikipedia', ''))\n",
    "    return stations, longitudes, latitudes\n",
    "\n",
    "mstation, longitude, latitude = mstation_location(links(html))\n",
    "\n",
    "# Parsing location data in degrees with decimals\n",
    "def dms2dd(degrees, minutes, seconds, direction):\n",
    "    dd = float(degrees) + float(minutes)/60 + float(seconds)/(60*60);\n",
    "    if direction == 'E' or direction == 'N':\n",
    "        dd *= -1\n",
    "    return dd;\n",
    "\n",
    "def dd2dms(deg):\n",
    "    d = int(deg)\n",
    "    md = abs(deg - d) * 60\n",
    "    m = int(md)\n",
    "    sd = (md - m) * 60\n",
    "    return [d, m, sd]\n",
    "\n",
    "def parse_dms(dms):\n",
    "    parts = re.split('[°′″]+', str(dms))\n",
    "    lat = dms2dd(parts[0], parts[1], parts[2], parts[3])\n",
    "    return (lat)\n",
    "\n",
    "# Defining the coordinates for the location of the stations\n",
    "latitude = [parse_dms(lat[0]) for lat in latitude]\n",
    "longitude = [parse_dms(long[0]) for long in longitude]\n",
    "\n",
    "# Convert and merge into a pandas dataframe\n",
    "df_location = pd.DataFrame([mstation, longitude, latitude]).T\n",
    "df_location.columns = ['Station', 'Longitude', 'Latitude']\n",
    "df_mstation = pd.merge(table, df_location, on='Station')\n",
    "\n",
    "# store the metro stations\n",
    "file_path = dir + \"/boliga/data/mstation_data.csv\"\n",
    "with open(file_path, mode='w', encoding='UTF-8',\n",
    "              errors='strict', buffering=1) as f:\n",
    "    f.write(df_mstation.to_csv())\n",
    "\n",
    "####################\n",
    "# S-train stations #\n",
    "####################\n",
    "\n",
    "# Scrape initial table of stations\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_Copenhagen_S-train_stations\"\n",
    "resp, callid = connector.get(url, 'sstation_scrape')\n",
    "html = resp.text\n",
    "\n",
    "# Parse table\n",
    "table = pd.read_html(resp.text)[1]\n",
    "table = table.drop(['Transfer', 'Line'], axis = 1)\n",
    "table['Station'] = table['Station'].str.translate({ord('#'): '', ord('†'): ''})\n",
    "\n",
    "# Function whih gets lontitutes and latitudes of stations\n",
    "def sstation_location(urls):\n",
    "    stations = []\n",
    "    longitudes = []\n",
    "    latitudes = []\n",
    "    for station in urls:\n",
    "        html = requests.get(station).text\n",
    "        s = BeautifulSoup(html, 'lxml')\n",
    "        loc = s.select('span.geo-dms')[0]\n",
    "        latFmt = re.compile('.*latitude\">(.*)</span> <')\n",
    "        lonFmt = re.compile('.*longitude\">(.*)</span><')\n",
    "        station = str(s.title.string).replace(' Station - Wikipedia', '')\n",
    "        station = station.replace(' station - Wikipedia', '')\n",
    "        #appending to containers\n",
    "        longitudes.append(lonFmt.findall(str(loc)))\n",
    "        latitudes.append(latFmt.findall(str(loc)))\n",
    "        stations.append(station)\n",
    "    return stations, longitudes, latitudes\n",
    "\n",
    "sstation, longitude, latitude = sstation_location(links(html))\n",
    "\n",
    "# Defining the coordinates for the location of the stations\n",
    "latitude = [parse_dms(lat[0]) for lat in latitude]\n",
    "longitude = [parse_dms(long[0]) for long in longitude]\n",
    "\n",
    "# Convert and merge into a pandas dataframe\n",
    "df_location = pd.DataFrame([sstation, longitude, latitude]).T\n",
    "df_location.columns = ['Station', 'Longitude', 'Latitude']\n",
    "df_sstation = pd.merge(table, df_location, on='Station')\n",
    "\n",
    "# Shows the s-train stations\n",
    "file_path = dir + \"/boliga/data/sstation_data.csv\"\n",
    "with open(file_path, mode='w', encoding='UTF-8',\n",
    "              errors='strict', buffering=1) as f:\n",
    "    f.write(df_sstation.to_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random sample inspection of our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample of the data scraped from boliga.dk\n",
    "print(boliga_data.sample(15))\n",
    "\n",
    "# Random sample of data from the metro stations\n",
    "print(df_mstation.sample(5))\n",
    "\n",
    "# Random sample of data from the s-train stations\n",
    "print(df_sstation.sample(5))\n",
    "\n",
    "# Checks if our scraped data contains duplicates\n",
    "# Boliga constains 14.400 sold houses in the chosen period of time (01/01/1995 - 31/12/2007)\n",
    "print(len(boliga_data), len(boliga_data.drop_duplicates()))\n",
    "\n",
    "# Shows the amount of different real estate types (appartments, houses, terraced house)\n",
    "print(boliga_data['Type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load boliga_data\n",
    "dir = str(os.getcwd())\n",
    "datafile = dir + \"/boliga/data/housing_data.csv\"\n",
    "boliga_data = pd.read_csv(datafile)\n",
    "boliga_data = boliga_data.iloc[:, 2:]\n",
    "boliga_data['building_address'] = pd.DataFrame([a[0] + ' ' + a[-1][6:]\\\n",
    "                for a in boliga_data['Address'].str.split(',')])\n",
    "boliga_data['building_address'] = boliga_data['building_address'].str.lstrip()\n",
    "\n",
    "# load appended location data\n",
    "location_file = dir +\"/boliga/data/location_data.csv\"\n",
    "location_data = pd.read_csv(location_file)\n",
    "location_data = location_data.iloc[:,2:]\n",
    "\n",
    "#merge data\n",
    "full_data = pd.merge(boliga_data, location_data, on= 'building_address')\n",
    "full_data = full_data.reset_index()\n",
    "full_data = full_data.drop(['index', 'building_address'], axis=1)\n",
    "print(full_data.head())\n",
    "\n",
    "file_path = str(dir + '/boliga/data/full_data.csv')\n",
    "with open(file_path, mode='w', encoding='UTF-8',\n",
    "          errors='strict', buffering=1) as f:\n",
    "    f.write(full_data.to_csv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading analysis data from full data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading in csv-files\n",
    "dir = str(os.getcwd())\n",
    "path_full = dir +'/boliga/data/full_data.csv'\n",
    "path_msta = dir +'/boliga/data/mstation_data.csv'\n",
    "path_ssta = dir +'/boliga/data/sstation_data.csv'\n",
    "full_data = pd.read_csv(path_full, index_col=0)\n",
    "df_mstation = pd.read_csv(path_msta, index_col=0)\n",
    "#Correcting the negative values to a positiv\n",
    "df_mstation[['Longitude','Latitude']] = df_mstation[['Longitude','Latitude']].abs()\n",
    "df_sstation = pd.read_csv(path_ssta, index_col=0)\n",
    "#Correcting the negative values to a positiv\n",
    "df_sstation[['Longitude', 'Latitude']] = df_sstation[['Longitude','Latitude']].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis\n",
    "In this section, the code for the data quality, descriptive statistics, the analysis is to be found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample identification\n",
    "#sample should be from befor 2008\n",
    "full_data = full_data.loc[full_data['Date_of_sale'] <= '2007-12-31']\n",
    "\n",
    "# deleting outliers\n",
    "full_data = full_data.loc[full_data['sqm_price']<80000]\n",
    "full_data = full_data.loc[full_data['Sell_price']<10000000]\n",
    "print('missingness on address: ',round((full_data.iloc[:, -1].isna().mean())*100,2), \"%\")\n",
    "print('missingness on address: ',full_data.iloc[:, -1].isna().sum())\n",
    "full_data = full_data[full_data['longitude']>12]\n",
    "\n",
    "full_data = full_data.sort_values('Date_of_sale')\n",
    "full_data = full_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sample of the data scraped from boliga.dk\n",
    "print(\"random sample of final data, \\n\",full_data.sample(100))\n",
    "\n",
    "# Random sample of data from the metro stations\n",
    "print(\"m-train station, \\n\", df_mstation)\n",
    "\n",
    "# Random sample of data from the s-train stations\n",
    "print(\"s-train station, \\n\", df_sstation)\n",
    "\n",
    "# Checking for duplicates\n",
    "print(\"checking for duplicates: \",len(full_data), len(full_data.drop_duplicates()))\n",
    "\n",
    "# Shows the amount of different real estate types (appartments, houses, terraced house)\n",
    "print(\"count py real estate types: \", full_data['Type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualization of log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the logfile as a pandas dataframe\n",
    "log_df = pd.read_csv('log_boliga.csv', sep=';')\n",
    "log_df = log_df.iloc[:,:-2]\n",
    "log_df = log_df.drop(['response_size'], axis=1)\n",
    "log_df['dt'] = pd.to_datetime(log_df['t'], unit= 's')\n",
    "log_df.sort_values('id')\n",
    "log_df.columns = ['id', 'project', 'connector_type', 't', 'delta_t', 'url',\n",
    "                  'response_size', 'response_code', 'datetime' ]\n",
    "print(\"head of log_file: \\n\", log_df.head())\n",
    "\n",
    "# Visualization of the time it took to make the call for data\n",
    "plt.figure(figsize = (13, 4))\n",
    "plt.plot(log_df['datetime'], log_df.delta_t, color='green')\n",
    "plt.ylabel('Delta t', color='black')\n",
    "plt.xlabel('Scraping process', color='black')\n",
    "plt.title('a: The time it took to make the call for data', weight='bold')\n",
    "plt.savefig('fig4.png')\n",
    "\n",
    "# Visualization of the response size through the scraping process\n",
    "plt.figure(figsize = (6.5, 4))\n",
    "plt.plot(log_df['datetime'], log_df['response_size'], color='green')\n",
    "plt.ylabel('bytes in the csv files', color='black')\n",
    "plt.xlabel('Scraping process', color='black')\n",
    "plt.title('b: The size of csv files through the scraping process', weight='bold')\n",
    "plt.savefig('fig5.png')\n",
    "\n",
    "# Plot the delta_t against the response_size - to see correlation.\n",
    "fig, ax = plt.subplots(figsize = (5, 4))\n",
    "ax.scatter(log_df.delta_t.astype('float'), log_df.response_size, color = 'green')\n",
    "ax.fmt_xdata = mdates.DateFormatter('%s-%f')\n",
    "plt.ylabel('')\n",
    "plt.xlabel('Delta t', color='black')\n",
    "plt.title('c: assocation between Delta t and file size', weight='bold')\n",
    "plt.savefig('fig6.png')\n",
    "\n",
    "q_cols = log_df.loc[:, ['delta_t', 'response_size']]\n",
    "print(q_cols.corr(method='pearson'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalsum = np.sum(full_data['Type'].value_counts())\n",
    "ListOfTypes = ['Ejerlejlighed', 'Villa', 'Rækkehus','Fritidshus']\n",
    "for i in ListOfTypes:\n",
    "    cd = full_data['Type'].value_counts()\n",
    "    for i in cd:\n",
    "        percent_type = round(cd/totalsum ,3)\n",
    "\n",
    "percent_df = pd.DataFrame(percent_type)\n",
    "\n",
    "# Rename columns\n",
    "percent_df = percent_df.rename(columns={'Type':'Share of total'})\n",
    "print(\"descriptive statistics: \\n\", percent_df)\n",
    "\n",
    "# Histogram of the distrubution of square meters\n",
    "fig1 = plt.hist(full_data['m2'], color = 'green', edgecolor = 'black', bins=40, linewidth=1.2)\n",
    "fig1 = plt.title('a: Size of sold real-estate properties', weight='bold')\n",
    "fig1 = plt.xlabel('Square meters', color='black')\n",
    "fig1 = plt.ylabel('Observations', color='black')\n",
    "fig1 = plt.xlim(0,420)\n",
    "fig1 = plt.ylim(0,3500)\n",
    "plt.savefig('fig1.png')\n",
    "\n",
    "# Rooms in sold properties\n",
    "fig2 = plt.hist(full_data['Rooms'], color = 'green', edgecolor='black', bins=20, linewidth=1.2)\n",
    "fig2 = plt.title('b: Rooms in sold real-estate properties', weight='bold')\n",
    "fig2 = plt.xlabel('Rooms', color='black')\n",
    "fig2 = plt.ylabel('Observations', color='black')\n",
    "fig2 = plt.xlim(0,12)\n",
    "fig2 = plt.ylim(0,5000)\n",
    "plt.savefig('fig2.png')\n",
    "\n",
    "# Building year of sold properties\n",
    "fig3 = plt.hist(full_data['Building_year'], bins=90, color= 'green', edgecolor='black', linewidth = 1.2)\n",
    "fig3 = plt.title('Building year of real-estate properties', weight='bold')\n",
    "fig3 = plt.xlabel('Year', color='black')\n",
    "fig3 = plt.ylabel('Observations', color='black')\n",
    "fig3 = plt.xlim(1600, 2030)\n",
    "fig3 = plt.ylim(0, 3500)\n",
    "plt.savefig('fig3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montly changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing DataFrame with monthly change\n",
    "dta = full_data['Date_of_sale']\n",
    "dta1 = pd.to_datetime(dta)\n",
    "merged1 = pd.concat([dta1, full_data['sqm_price']], axis=1)\n",
    "merged1 = merged1.set_index('Date_of_sale')\n",
    "\n",
    "monthly_price = round(merged1['sqm_price'].resample('M').mean(),2)\n",
    "monthly_price = monthly_price.to_frame()\n",
    "\n",
    "monthly_price['Monthly change'] = round(monthly_price.pct_change(periods=1, axis=0),2)\n",
    "def percent_change(df):\n",
    "    monthly_price['Total change'] = round((100 * (monthly_price.sqm_price/monthly_price.iloc[0].sqm_price -1)),2)\n",
    "    return monthly_price\n",
    "\n",
    "monthly_price.groupby('Date_of_sale').apply(percent_change)\n",
    "monthly_price.index = pd.to_datetime(monthly_price.index)\n",
    "\n",
    "#Figure 9\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(monthly_price['Monthly change'], color='green')\n",
    "plt.title('a: Monthly change in sqare meter price', weight = 'bold')\n",
    "plt.xlabel('Year', color='black')\n",
    "plt.ylabel('Change, %', color='black')\n",
    "fig.autofmt_xdate()\n",
    "ax.fmt_xdata = mdates.DateFormatter('%Y-%m-%d')\n",
    "plt.savefig('fig9.png')\n",
    "\n",
    "# Figure 10\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(monthly_price['Total change'], color ='green')\n",
    "plt.title('b: Total change in square meter price since 1995', weight='bold')\n",
    "plt.xlabel('Year', color='black')\n",
    "plt.ylabel('Change, %', color='black')\n",
    "fig.autofmt_xdate()\n",
    "ax.fmt_xdata = mdates.DateFormatter('%Y-%m-%d')\n",
    "plt.savefig('fig10.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table summerizing changes and averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly = round(merged1['sqm_price'].resample('Y').mean(),2)\n",
    "yearly = yearly.to_frame()\n",
    "\n",
    "# Average percentual change 1995-2002:\n",
    "av1 = round(((16979.78/6349.07)**(1/7)-1)*100,1)\n",
    "print(av1)\n",
    "\n",
    "#Average percentual change 2002 - 2007\n",
    "av2 = round(((28184.09/17691.70)**(1/4)-1)*100,1)\n",
    "print(av2)\n",
    "\n",
    "#Total\n",
    "av3 = round((28184.09/6349.07-1)*100,1)\n",
    "\n",
    "# Average percentual change 2003-2006\n",
    "av4 = round(((30660.32/17691.70)**(1/3)-1)*100,1)\n",
    "print(av3)\n",
    "print(av4)\n",
    "\n",
    "#Average square meters\n",
    "av5 = round(sum(full_data['m2'])/12542,1)\n",
    "print(av5)\n",
    "\n",
    "#Average rooms\n",
    "av6 = round(sum(full_data['Rooms'])/12542,1)\n",
    "\n",
    "table1 = [[\"1995-2002\",15.1,\"NaN\"],[\"2003-2007\",12.4,\"NaN\"],[\"2002-2006\",20.1,\"NaN\"],\n",
    "          [\"Total\",343.9,\"NaN\"], [\"Average sqm\", \"NaN\", 3.4],[\"Average rooms\", \"NaN\", 15.1]]\n",
    "print(tabulate(table1, headers=[\"pct. change\",\"Average\"], tablefmt=\"latex\"))\n",
    "\n",
    "ab2 = full_data.drop(['Unnamed: 0', 'Address','Date_of_sale', 'location','m_station_const','s_station','m_station','s_station_const','latitude','longitude','Building_year'], axis=1)\n",
    "a1 = round(ab2.describe().T,1)\n",
    "a1=a1.reset_index()\n",
    "\n",
    "#importing plotly\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "\n",
    "#making a folder called images:\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.mkdir(\"images\")\n",
    "    \n",
    "#making a table figure\n",
    "fig14 = go.Figure(data=[go.Table(columnwidth = [650,400],\n",
    "                                 header=dict(values=['Variable','Count','Mean','Std','Min','Max'], \n",
    "                                            line_color='black',\n",
    "                                            fill_color='lightgrey',\n",
    "                                            font=dict(color='black', size=14),\n",
    "                                            align=['left','center']),\n",
    "                                 cells=dict(values=[a1['index'],a1['count'],a1['mean'],a1['std'],a1['min'],a1['max']], \n",
    "                                            line_color='black',\n",
    "                                            fill=dict(color=['lightgrey', 'white']),\n",
    "                                            font = dict(color = 'Black', size = 12),\n",
    "                                            align=['left', 'center']))\n",
    "                       ])\n",
    "\n",
    "fig14.write_image(\"images/fig14.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample identification\n",
    "# Sample should be from befor 2008\n",
    "full_data = full_data.loc[full_data['Date_of_sale'] <= '2007-12-31']\n",
    "\n",
    "# Deleting outliers\n",
    "full_data = full_data.loc[full_data['sqm_price']<80000]\n",
    "full_data = full_data.loc[full_data['Sell_price']<10000000]\n",
    "print('missingness on address: ',round((full_data.iloc[:, -1].isna().mean())*100,2), \"%\")\n",
    "full_data = full_data[full_data['longitude']>12]\n",
    "\n",
    "full_data = full_data.sort_values('Date_of_sale')\n",
    "full_data = full_data.reset_index(drop=True)\n",
    "print(full_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating heatmap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folium map\n",
    "\n",
    "def map_points(df, lat_col='latitude', lon_col='longitude', zoom_start=11, \\\n",
    "                plot_points=False, pt_radius=2, \\\n",
    "                draw_heatmap=False, heat_map_weights_col=None, \\\n",
    "                heat_map_weights_normalize=True, heat_map_radius=15):\n",
    "    \"\"\"Creates a map given a dataframe of points. Can also produce a heatmap overlay\n",
    "    Arg:\n",
    "        df: dataframe containing points to maps\n",
    "        lat_col: Column containing latitude (string)\n",
    "        lon_col: Column containing longitude (string)\n",
    "        zoom_start: Integer representing the initial zoom of the map\n",
    "        plot_points: Add points to map (boolean)\n",
    "        pt_radius: Size of each point\n",
    "        draw_heatmap: Add heatmap to map (boolean)\n",
    "        heat_map_weights_col: Column containing heatmap weights\n",
    "        heat_map_weights_normalize: Normalize heatmap weights (boolean)\n",
    "        heat_map_radius: Size of heatmap point\n",
    "    Returns:\n",
    "        folium map object\n",
    "    \"\"\"\n",
    "\n",
    "    ## center map in the middle of points center in\n",
    "    middle_lat = df[lat_col].median()\n",
    "    middle_lon = df[lon_col].median()\n",
    "\n",
    "    curr_map = folium.Map(location=[middle_lat, middle_lon],\n",
    "                          zoom_start=zoom_start,\n",
    "                          tiles='Stamen Terrain')\n",
    "\n",
    "    # add points to map\n",
    "    if plot_points:\n",
    "        for _, row in df.iterrows():\n",
    "            folium.CircleMarker([row[lat_col], row[lon_col]],\n",
    "                                radius=pt_radius,\n",
    "                                popup=row['location'],\n",
    "                                fill_color=\"#3db7e4\", # divvy color\n",
    "                               ).add_to(curr_map)\n",
    "\n",
    "    # add heatmap\n",
    "    if draw_heatmap:\n",
    "        # convert to (n, 2) or (n, 3) matrix format\n",
    "        if heat_map_weights_col is None:\n",
    "            cols_to_pull = [lat_col, lon_col]\n",
    "        else:\n",
    "            # if we have to normalize\n",
    "            if heat_map_weights_normalize:\n",
    "                df[heat_map_weights_col] = \\\n",
    "                    df[heat_map_weights_col] / df[heat_map_weights_col].sum()\n",
    "\n",
    "            cols_to_pull = [lat_col, lon_col, heat_map_weights_col]\n",
    "\n",
    "        houses = df[cols_to_pull].values\n",
    "        curr_map.add_child(plugins.HeatMap(houses, radius=heat_map_radius,gradient={.4: 'green', .85:'yellow', 1: 'red'}))\n",
    "\n",
    "\n",
    "    return curr_map\n",
    "\n",
    "# define map\n",
    "# Creating a map of Copenhagen (specified to the area that we need)\n",
    "m = folium.Map([55.676098, 12.568337], zoom_start=12)\n",
    "\n",
    "# Adding s-train stations to the map\n",
    "for index, row in df_sstation.iterrows():\n",
    "    folium.Circle(\n",
    "        radius=50,\n",
    "        location=(row['Latitude'],row['Longitude']),\n",
    "        popup=row['Station'],\n",
    "        fill=True,\n",
    "        color = 'Blue').add_to(m)\n",
    "\n",
    "# Adding metro-stations to the map\n",
    "for index, row in df_mstation.iterrows():\n",
    "    folium.Circle(\n",
    "        radius=50,\n",
    "        location=(row['Latitude'],row['Longitude']),\n",
    "        popup=row['Station'],\n",
    "        fill=True,\n",
    "        color = 'Red').add_to(m)\n",
    "\n",
    "m\n",
    "\n",
    "# Adding the heatmap of sold properties to the map with s-train and metro-stations\n",
    "Sold_houses = full_data[['latitude', 'longitude']].values\n",
    "m.add_child(plugins.HeatMap(Sold_houses, radius=10))\n",
    "\n",
    "Train_legend =   '''\n",
    "                <div style=\"position: fixed; background:white;\n",
    "                            top: 150px; right: 350px; width: 170px; height: 75;\n",
    "                            border:3px solid grey; z-index:9999; font-size:16.5px; font-color=black; weight='bold';\n",
    "                            \">&nbsp; Station type: <br>\n",
    "                              &nbsp; Metro-station &nbsp; <i class=\"fa fa-circle-thin fa-1x\" style=\"color:red\"></i><br>\n",
    "                              &nbsp; S-train station &nbsp; <i class=\"fa fa-circle-thin fa-1x\" style=\"color:blue\"></i>\n",
    "                </div>\n",
    "                '''\n",
    "\n",
    "m.get_root().html.add_child(folium.Element(Train_legend))\n",
    "m.save('copenhagen_map.html')\n",
    "\n",
    "# using selenium to safe HTML as png\n",
    "path2gecko = '/Users/MacbookJos/git/geckodriver' # define path  geckodriver\n",
    "browser = webdriver.Firefox(executable_path=path2gecko)\n",
    "html= 'file:///Users/MacbookJos/git/sds_exam/copenhagen_map.html'\n",
    "\n",
    "browser.get(html)\n",
    "time.sleep(5)\n",
    "browser.save_screenshot('fig8.png')\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating list of neigbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = pd.DataFrame([a[0] + ' ' + a[-1][6:]\\\n",
    "           for a in full_data['Address'].str.split(',')])\n",
    "header=['ad']\n",
    "address.columns=header\n",
    "\n",
    "municipalities = ['Frederiksberg C', 'Frederiksberg', 'København K', 'København V', 'København S', 'København N', 'København Ø']\n",
    "\n",
    "address_list = []\n",
    "\n",
    "for i in address['ad']:\n",
    "    for x in municipalities:\n",
    "        if x in i:\n",
    "            address_list.append(x)\n",
    "            break\n",
    "\n",
    "# Making a list of municipalities\n",
    "Municipality = pd.DataFrame(address_list)\n",
    "Municipality.columns=['Municipality']\n",
    "\n",
    "# Adding the list to the housing data\n",
    "df['Municipality'] = Municipality\n",
    "df = df[['Municipality', 'Type']]\n",
    "df = pd.get_dummies(df)\n",
    "df.describe().round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #parse the opening year from int to string to datetime\n",
    "df_sstation['Opened'] = df_sstation['Opened'].apply(lambda x: str(x))\n",
    "df_mstation['Opened'] = df_mstation['Opened'].apply(lambda x: str(x))\n",
    "df_sstation['Opened'] = df_sstation['Opened'].apply(lambda x: parse(x))\n",
    "df_mstation['Opened'] = df_mstation['Opened'].apply(lambda x: parse(x))\n",
    "#parse the date of sale from to datetime\n",
    "full_data['Date_of_sale'] = full_data['Date_of_sale'].apply(lambda x: str(x))\n",
    "full_data['Date_of_sale'] = full_data['Date_of_sale'].apply(lambda x: parse(x))\n",
    "\n",
    "#find which stations were open at the year of the sale\n",
    "def was_opened(property_, stations):\n",
    "    open_stations = {}\n",
    "    for i in range(len(stations)):\n",
    "        if (stations.iat[i,2] < property_[2]) == True:\n",
    "            open_stations[stations.iat[i,0]] = (abs(stations.iat[i,-1]), abs(stations.iat[i,-2]))\n",
    "    return open_stations\n",
    "\n",
    "#calculate distance to closeste open station\n",
    "def get_distance_opened(property_, df_stations):\n",
    "    stations = was_opened(property_, df_stations)\n",
    "    if  stations == {}:\n",
    "        return None, None\n",
    "    property_loc = (property_[-2], property_[-1])\n",
    "    if property_loc[0] is not None or property_loc[1] is not None:\n",
    "        min_dist = 999999999999999999999\n",
    "        min_dist_station = ''\n",
    "        for station, station_loc in stations.items():\n",
    "            dist = geodesic(station_loc, property_loc).km\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_dist_station = station\n",
    "        return round(min_dist,5), min_dist_station\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# find which stations which were not open during the time of the sale \n",
    "def was_not_opened(property_, stations):\n",
    "    open_stations = {}\n",
    "    for i in range(len(stations)):\n",
    "         if (stations.iat[i,2] > property_[2]) == True:\n",
    "            open_stations[stations.iat[i,0]] = (abs(stations.iat[i,-1]),abs(stations.iat[i,-2]))\n",
    "    return open_stations\n",
    "\n",
    "#Calculates the distance to the closest station under construction\n",
    "def get_distance_not_opened(property_, df_stations):\n",
    "    stations = was_not_opened(property_, df_stations)\n",
    "    if  stations == {}:\n",
    "        return None, None\n",
    "    property_loc = (property_[-2], property_[-1])\n",
    "    if property_loc[0] is not None or property_loc[1] is not None:\n",
    "        min_dist = 999999999999999999999\n",
    "        min_dist_station = ''\n",
    "        for station, station_loc in stations.items():\n",
    "            dist = geodesic(station_loc, property_loc).km\n",
    "            if dist < min_dist:\n",
    "                min_dist = dist\n",
    "                min_dist_station = station\n",
    "        return round(min_dist,3), min_dist_station\n",
    "     else:\n",
    "        return None, None\n",
    "\n",
    "#calculates the distance to the city center     \n",
    "geolocator = Nominatim(user_agent=\"Social Data Science Student\", timeout =50)\n",
    "copenhagen = geolocator.geocode(\"Copenhagen\")[-1]\n",
    "def dist_city_center(property_):\n",
    "    property_loc = (property_[-2], property_[-1])\n",
    "    if property_loc[0] is not None or property_loc[1] is not None:\n",
    "        return round(geodesic(property_loc, copenhagen).km, 3)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# construct distance objects\n",
    "distance_m = full_data.apply(lambda x: get_distance_opened(x, df_mstation), axis=1)\n",
    "distance_s = full_data.apply(lambda x: get_distance_opened(x, df_sstation), axis=1)\n",
    "distance_m_const = full_data.apply(lambda x: get_distance_not_opened(x, df_mstation), axis=1)\n",
    "distance_s_const = full_data.apply(lambda x: get_distance_not_opened(x, df_sstation), axis=1)\n",
    "distance_c = full_data.apply(dist_city_center, axis=1)\n",
    "\n",
    "#adding the distances to the dataframe\n",
    "full_data['m_distance'], full_data['m_station'] = [m[0]for m in distance_m], [m[1]for m in distance_m]\n",
    "full_data['s_distance'], full_data['s_station'] = [s[0]for s in distance_s], [s[1]for s in distance_s]\n",
    "full_data['m_distance_const'], full_data['m_station_const'] = [m[0]for m in distance_m_const], [m[1]for m in distance_m_const]\n",
    "full_data['s_distance_const'], full_data['s_station_const'] = [s[0]for s in distance_s_const], [s[1]for s in distance_s_const]\n",
    "full_data['c_distance'] = distance_c\n",
    "\n",
    " # safe file\n",
    "file_path = dir + \"/boliga/data/analysis_data.csv\"\n",
    "with open(file_path, mode='w', encoding='UTF-8',\n",
    "              errors='strict', buffering=1) as f:\n",
    "    f.write(full_data.to_csv())\n",
    "path_analysis = dir +'/boliga/data/analysis_data.csv'\n",
    "full_data = pd.read_csv(path_analysis, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction a percentage difference from  a rolling mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['z_sqm_price'] = (full_data.sqm_price -\\\n",
    "                            full_data.sqm_price.rolling(window=30).mean())\\\n",
    "                            / full_data.sqm_price.rolling(window=30).mean()*100\n",
    " # safe file\n",
    "file_path = dir + \"/boliga/data/analysis_data.csv\"\n",
    "with open(file_path, mode='w', encoding='UTF-8',\n",
    "              errors='strict', buffering=1) as f:\n",
    "    f.write(full_data.to_csv())\n",
    "path_analysis = dir +'/boliga/data/analysis_data.csv'\n",
    "full_data = pd.read_csv(path_analysis, index_col=0)\n",
    "\n",
    "# ensuring date time as column type\n",
    "full_data['Date_of_sale'] = full_data['Date_of_sale'].apply(lambda x: str(x))\n",
    "full_data['Date_of_sale'] = full_data['Date_of_sale'].apply(lambda x: parse(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make distance plot\n",
    "datafile = dir + \"/boliga/data/analysis_data.csv\"\n",
    "df = pd.read_csv(datafile, index_col=0)\n",
    "df = df[['z_sqm_price', 'm_distance']]\n",
    "df = df.dropna()\n",
    "df.sort_values('z_sqm_price')\n",
    "\n",
    "x = df['m_distance'].to_numpy()\n",
    "y = df['z_sqm_price'].to_numpy()\n",
    "sorted_indices = np.argsort(x)\n",
    "sorted_x = x[sorted_indices]\n",
    "\n",
    "l = loess(x, y, frac=0.1)\n",
    "l.fit()\n",
    "pred = l.predict(sorted_x, stderror=True)\n",
    "conf = pred.confidence()\n",
    "\n",
    "lowess = pred.values\n",
    "ll = conf.lower\n",
    "ul = conf.upper\n",
    "\n",
    "# plt.plot(x, y, '+')\n",
    "fig, ax1 = plt.subplots( figsize=(13, 4))\n",
    "ax1.plot(sorted_x, lowess, color='green')\n",
    "ax1.fill_between(sorted_x,ll,ul,alpha=.15, color='green')\n",
    "ax1.set_ylabel('%-difference in square meter price from rolling mean')\n",
    "ax.set_xlabel(\"distance in kilometers\")\n",
    "ax1.set_xlim(0,5)\n",
    "plt.savefig('fig7.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making opening plot points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of opning years for all metro stations\n",
    "m2002 = ['Sundby', 'Ørestad', 'Nørreport', 'Lergravsparken', 'Kongens Nytorv', 'Islands Brygge', 'DR Byen', 'Bella Center', 'Amagerbro']\n",
    "m2003 = ['Fasanvej', 'Forum', 'Frederiksberg', 'Lindevang']\n",
    "m2004 = ['Flintholm']\n",
    "m2007 = ['Amager Strand', 'Øresund','Femøren', 'Kastrup']\n",
    "\n",
    "# making new dataframes for each opening year for those sold after metro opening\n",
    "open2002 = full_data.loc[full_data['m_station'].isin(m2002)]\n",
    "open2003 = full_data.loc[full_data['m_station'].isin(m2003)]\n",
    "open2004 = full_data.loc[full_data['m_station'].isin(m2004)]\n",
    "open2007 = full_data.loc[full_data['m_station'].isin(m2007)]\n",
    "\n",
    "# making new dataframes for each opening year for those sold before metro opening\n",
    "open2002_c = full_data.loc[full_data['m_station_const'].isin(m2002)]\n",
    "open2003_c = full_data.loc[full_data['m_station_const'].isin(m2003)]\n",
    "open2004_c = full_data.loc[full_data['m_station_const'].isin(m2004)]\n",
    "open2007_c = full_data.loc[full_data['m_station_const'].isin(m2007)]\n",
    "\n",
    "open2002_c['m_distance'] = open2002_c['m_distance_const']\n",
    "open2003_c['m_distance'] = open2003_c['m_distance_const']\n",
    "open2004_c['m_distance'] = open2004_c['m_distance_const']\n",
    "open2007_c['m_distance'] = open2007_c['m_distance_const']\n",
    "\n",
    "# join the two data set \n",
    "open2002 = open2002.append(open2002_c, ignore_index=True)\n",
    "open2003 = open2003.append(open2003_c, ignore_index=True)\n",
    "open2004 = open2004.append(open2004_c, ignore_index=True)\n",
    "open2007 = open2007.append(open2007_c, ignore_index=True)\n",
    "\n",
    "#function to group distances in three categories\n",
    "def group_dist(dist):\n",
    "    if dist < 0.5:\n",
    "        x = 'Less than 0.5 km'\n",
    "    elif dist >= 0.5 and dist <= 1:\n",
    "        x = '0.5 -1 km'\n",
    "    else:\n",
    "        x = 'Above 1 km'\n",
    "    return x\n",
    "\n",
    "#grouping distances \n",
    "dist_group2002 = open2002['m_distance'].apply(lambda x: group_dist(x))\n",
    "dist_group2003 = open2003['m_distance'].apply(lambda x: group_dist(x))\n",
    "dist_group2004 = open2004['m_distance'].apply(lambda x: group_dist(x))\n",
    "dist_group2007 = open2007['m_distance'].apply(lambda x: group_dist(x))\n",
    "open2002.insert(1, 'Grouped_distance', dist_group2002)\n",
    "open2003.insert(1, 'Grouped_distance', dist_group2003)\n",
    "open2004.insert(1, 'Grouped_distance', dist_group2004)\n",
    "open2007.insert(1, 'Grouped_distance', dist_group2007)\n",
    "\n",
    "#getting data per year\n",
    "open2002['Date_of_sale'] = open2002['Date_of_sale'].apply(lambda x: x.year)\n",
    "open2003['Date_of_sale'] = open2003['Date_of_sale'].apply(lambda x: x.year)\n",
    "open2004['Date_of_sale'] = open2004['Date_of_sale'].apply(lambda x: x.year)\n",
    "open2007['Date_of_sale'] = open2007['Date_of_sale'].apply(lambda x: x.year)\n",
    "\n",
    "\n",
    "#creating plot for all four years \n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, figsize=(13, 20))\n",
    "plt.tight_layout(pad=3, w_pad=2.5, h_pad=2)\n",
    "sns.lineplot(data=open2002, x='Date_of_sale', y='z_sqm_price', hue='Grouped_distance', palette='Greens', ax=ax1)\n",
    "ax1.axvline(2002, color='k', linestyle=':')\n",
    "ax1.set_xlabel(\"Year which property was sold\")\n",
    "ax1.set_ylabel(\"%-difference in square meter price from rolling mean\")\n",
    "ax1.margins(0)\n",
    "sns.lineplot(data=open2003, x='Date_of_sale', y='z_sqm_price', hue='Grouped_distance', palette='Greens', ax=ax2)\n",
    "ax3.axvline(2004, color='k', linestyle=':')\n",
    "ax3.set_xlabel(\"Year which property was sold\")\n",
    "ax3.set_ylabel(\"%-difference in square meter price from rolling mean\")\n",
    "ax3.margins(0)\n",
    "sns.lineplot(data=open2004, x='Date_of_sale', y='z_sqm_price', hue='Grouped_distance', palette='Greens', ax=ax3)\n",
    "ax2.axvline(2003, color='k', linestyle=':')\n",
    "ax2.set_xlabel(\"Year which property was sold\")\n",
    "ax2.set_ylabel(\"%-difference in square meter price from rolling mean\")\n",
    "ax2.margins(0)\n",
    "sns.lineplot(data=open2007, x='Date_of_sale', y='z_sqm_price', hue='Grouped_distance', palette='Greens', ax=ax4)\n",
    "ax4.axvline(2007, color='k', linestyle=':')\n",
    "ax4.set_xlim(1998,2008)\n",
    "ax4.set_xlabel(\"Year which property was sold\")\n",
    "ax4.set_ylabel(\"%-difference in square meter price from rolling mean\")\n",
    "ax4.legend(loc=1)\n",
    "ax4.margins(0)\n",
    "plt.savefig('fig11.png')\n",
    "\n",
    "#creating plot for 2002 \n",
    "fig, ax = plt.subplots(figsize = (13, 4))\n",
    "sns.lineplot(data=open2002, x='Date_of_sale', y='z_sqm_price', hue='Grouped_distance', palette='Greens', ax=ax)\n",
    "# plt.title('%-difference in square meter price time from opening date', weight=\"bold\")\n",
    "ax.axvline(2002, color='k', linestyle=':')\n",
    "ax.legend(loc=4)\n",
    "ax.set_xlabel(\"Year which property was sold\")\n",
    "ax.set_ylabel(\"%-difference in square meter price from rolling mean\")\n",
    "plt.ylim(-30,20)\n",
    "plt.margins(0)\n",
    "plt.savefig('fig11_2002.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a new column for minimum distance for any station\n",
    "full_data['any_distance'] = full_data[['m_distance', 's_distance']].apply((lambda x: min(x)), axis=1)\n",
    "\n",
    "#create new columns at our dataframe which check the distance to the nearest station\n",
    "full_data['less_than_500m_mstation'] = full_data['m_distance'] <= 0.5\n",
    "full_data['less_than_500m_sstation'] = full_data['s_distance'] <= 0.5\n",
    "full_data['less_than_500m_any_station'] = full_data['any_distance'] <= 0.5\n",
    "full_data['more_than_500m_less_than_1km_mstation'] = ((full_data['m_distance'] > 0.5) & (full_data['m_distance'] <= 1.0))\n",
    "full_data['more_than_500m_less_than_1km_sstation'] = ((full_data['s_distance'] > 0.5) & (full_data['s_distance'] <= 1.0))\n",
    "full_data['more_than_500m_less_than_1km_any_station'] = ((full_data['any_distance'] > 0.5) & (full_data['any_distance'] <= 1.0)\n",
    "full_data['more_than_1km_mstation'] = full_data['m_distance'] > 1\n",
    "full_data['more_than_1km_sstation'] = full_data['s_distance'] > 1\n",
    "full_data['more_than_1km_any_station'] = full_data['any_distance'] > 1\n",
    "\n",
    "#keep only data after 2002; the year that the first metro stations were built\n",
    "data_after_2002 = full_data[full_data['Date_of_sale'] >= '2002-01-01']\n",
    "                                                         \n",
    "#creates 2 new columns in our dataframe which divide the houses in three categories (1,2,3) \n",
    "mdist = []\n",
    "sdist = []\n",
    "adist = []                                               \n",
    "for i in range(len(data_after_2002['less_than_500m_mstation'])):\n",
    "    #checking distance to mstations and creates a list of 3 categories\n",
    "    if data_after_2002['less_than_500m_mstation'].iloc[i] == True:\n",
    "        mdist.append(1)   \n",
    "    elif data_after_2002['more_than_500m_less_than_1km_mstation'].iloc[i] == True:\n",
    "        mdist.append(2)\n",
    "    else:\n",
    "        mdist.append(3)\n",
    "        \n",
    "    #checking distance to sstations and creates a list of 3 categories\n",
    "    if data_after_2002['less_than_500m_sstation'].iloc[i] == True:\n",
    "        sdist.append(1)   \n",
    "    elif data_after_2002['more_than_500m_less_than_1km_sstation'].iloc[i] == True:\n",
    "        sdist.append(2)\n",
    "    else:\n",
    "        sdist.append(3)\n",
    "                                                         \n",
    "    #checking distance to any_stations and creates a list of 3 categories\n",
    "    if data_after_2002['less_than_500m_any_station'].iloc[i] == True:\n",
    "        sdist.append(1)   \n",
    "    elif data_after_2002['more_than_500m_less_than_1km_any_station'].iloc[i] == True:\n",
    "        sdist.append(2)\n",
    "    else:\n",
    "        sdist.append(3)\n",
    "\n",
    "\n",
    "\n",
    "#depending on their distance from the nearest station\n",
    "data_after_2002['R_to_mstation'] = mdist\n",
    "data_after_2002['R_to_sstation'] = sdist\n",
    "data_after_2002['R_to_any_station'] = adist \n",
    "\n",
    "                                                        \n",
    "# creating boxplots for metro station\n",
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "sns.boxplot(x = 'R_to_mstation', y = 'sqm_price', data = data_after_2002, palette=\"Greens\");\n",
    "plt.xlabel('Residence distance to M-station', fontsize=10);\n",
    "plt.ylabel('sqm price (DKK)', fontsize=10);\n",
    "\n",
    "plt.xticks([0, 1, 2], ['r < 0.5 km', 'r > 0.5km and r < 1 km', 'r > 1 km']);\n",
    "f.tight_layout()\n",
    "f.savefig('sqm_distance_mstations.png');\n",
    "\n",
    "# creating boxplots for S-train stations                                                                                     \n",
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "sns.boxplot(x = 'R_to_sstation', y = 'sqm_price', data = data_after_2002, palette = 'Greens');\n",
    "\n",
    "plt.xlabel('Residence distance to S-station', fontsize=10);\n",
    "plt.ylabel('sqm price (DKK)', fontsize=10);\n",
    "\n",
    "plt.xticks([0, 1, 2], ['r < 0.5 km', 'r > 0.5km and r < 1 km', 'r > 1 km']);\n",
    "f.tight_layout()\n",
    "f.savefig('sqm_distance_sstations.png');\n",
    "                                                         \n",
    "# creating boxplot for any stations                                                         \n",
    "f, ax = plt.subplots(figsize=(10,6))\n",
    "sns.boxplot(x = 'R_to_any_station', y = 'sqm_price', data = data_after_2002, palette = 'Greens');\n",
    "\n",
    "plt.xlabel('Residence distance to any station', fontsize=10);\n",
    "plt.ylabel('sqm price (DKK)', fontsize=10);\n",
    "\n",
    "plt.xticks([0, 1, 2], ['r < 0.5 km', 'r > 0.5km and r < 1 km', 'r > 1 km']);\n",
    "f.tight_layout()\n",
    "f.savefig('sqm_distance_any_stations.png');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = str(os.getcwd())\n",
    "datafile = dir + \"/boliga/data/analysis_data.csv\"\n",
    "df = pd.read_csv(datafile, index_col=0)\n",
    "df = df[df['Date_of_sale'] > '2002']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a list of municipalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = str(os.getcwd())\n",
    "datafile = dir + \"/boliga/data/analysis_data.csv\"\n",
    "df = pd.read_csv(datafile, index_col=0)\n",
    "\n",
    "#seperating municipality\n",
    "address = pd.DataFrame([a[0] + ' ' + a[-1][6:]\\\n",
    "           for a in df['Address'].str.split(',')])\n",
    "header=['ad']\n",
    "address.columns=header\n",
    "\n",
    "municipalities = ['Frederiksberg C', 'Frederiksberg', 'København K', 'København V', 'København S', 'København N', 'København Ø']\n",
    "\n",
    "address_list = []\n",
    "\n",
    "for i in address['ad']: \n",
    "    for x in municipalities:\n",
    "        if x in i:\n",
    "            address_list.append(x)\n",
    "            break\n",
    "\n",
    "# Making a list of municipalities            \n",
    "Municipality = pd.DataFrame(address_list)\n",
    "Municipality.columns=['Municipality']\n",
    "\n",
    "# Adding the list to the housing data\n",
    "df['Municipality'] = Municipality\n",
    "df\n",
    "df = df.drop(['Sell_price', 'sqm_price','Date_of_sale','Address', 'location', 'latitude', 'longitude','m_distance_const', 'm_station_const', 's_distance_const',\n",
    "       's_station_const', 's_station', 'm_station'], axis=1).dropna();\n",
    "df = pd.get_dummies(df)\n",
    "df = df.reset_index(drop=True)\n",
    "y = df.z_sqm_price.to_numpy()\n",
    "X = df.drop(['z_sqm_price', 'Type_ Villa ', 'Municipality_København K'], axis=1).to_numpy()\n",
    "X = np.array(X, dtype=np.float64)\n",
    "X, y;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into test and train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into development (2/3) and test data (1/3)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X, y, test_size=1/3, random_state=34)\n",
    "\n",
    "# splitting development into train (1/3) and validation (1/3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_dev, y_dev, test_size=1/2, random_state=52)\n",
    "\n",
    "lambda_ = np.logspace(-1, 1, 22)\n",
    "l1_ratio_ = np.logspace(0, 3, 22)\n",
    "tol = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_OLS = make_pipeline(PolynomialFeatures(degree = 3, include_bias=True), \n",
    "                           StandardScaler(),\n",
    "                           LinearRegression())\n",
    "                         \n",
    "pipe_OLS.fit(X_dev, y_dev)\n",
    "print(mse(pipe_OLS.predict(X_test),y_test), round(np.sqrt(mse(pipe_OLS.predict(X_test),y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lasso = make_pipeline(PolynomialFeatures(degree = 3, include_bias=True), \n",
    "                           StandardScaler(),\n",
    "                           Lasso(max_iter = 10000, alpha=lambda_, tol=tol))\n",
    "\n",
    "train_scores, test_scores = validation_curve(estimator=pipe_lasso,\n",
    "                                             X=X_train, y=y_train,\n",
    "                                             param_name='lasso__alpha',\n",
    "                                             param_range=lambda_,\n",
    "                                             scoring='neg_mean_squared_error',\n",
    "                                             cv=5)\n",
    "\n",
    "mse_score = pd.DataFrame({'Train':-train_scores.mean(axis=1),\n",
    "                          'Validation':-test_scores.mean(axis=1),\n",
    "                          'lambda':lambda_}).set_index('lambda')   \n",
    "\n",
    "optimal_lambda_lasso = mse_score.Validation.nsmallest(1)\n",
    "lambda_o_ = optimal_lambda_lasso.index\n",
    "\n",
    "#testing\n",
    "pipe_lasso = make_pipeline(PolynomialFeatures(degree = 3, include_bias=True), \n",
    "                           StandardScaler(),\n",
    "                           Lasso(max_iter = 10000, alpha=optimal_lambda_lasso.index, tol=tol))\n",
    "\n",
    "pipe_lasso.fit(X_dev, y_dev)\n",
    "print(lambda_o_,' ',round(mse(pipe_lasso.predict(X_test),y_test),3), round(np.sqrt(mse(pipe_lasso.predict(X_test),y_test)),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_ridge = make_pipeline(PolynomialFeatures(degree = 3, include_bias=True), \n",
    "                           StandardScaler(),\n",
    "                           Ridge(alpha = l1_ratio_, tol=tol))\n",
    "\n",
    "train_scores, test_scores = validation_curve(estimator=pipe_ridge,\n",
    "                                             X=X_train, y=y_train,\n",
    "                                             param_name='ridge__alpha',\n",
    "                                             param_range=l1_ratio_,\n",
    "                                             scoring='neg_mean_squared_error',\n",
    "                                             cv=5)\n",
    "\n",
    "mse_score = pd.DataFrame({'Train':-train_scores.mean(axis=1),\n",
    "                          'Validation':-test_scores.mean(axis=1),\n",
    "                          'lambda':l1_ratio_}).set_index('lambda')   \n",
    "\n",
    "lambda_o_ = mse_score.Validation.nsmallest(1)\n",
    "lambda_o_ = lambda_o_.index\n",
    "\n",
    "\n",
    "#testing\n",
    "pipe_ridge = make_pipeline(PolynomialFeatures(degree = 3, include_bias=True), \n",
    "                           StandardScaler(),\n",
    "                           Ridge(alpha = lambda_o_, tol=tol))\n",
    "\n",
    "pipe_ridge.fit(X_dev, y_dev)\n",
    "print(lambda_o_,' ', round(mse(pipe_ridge.predict(X_test),y_test),3), round(np.sqrt(mse(pipe_ridge.predict(X_test),y_test)),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = KFold(n_splits=10)\n",
    "folds = list(kfolds.split(X_dev, y_dev))\n",
    "\n",
    "# outer loop: lambdas\n",
    "mseCV = []\n",
    "for lambda__ in lambda_:    \n",
    "    # inner loop: folds\n",
    "    mseCV_ = []    \n",
    "    for train_idx, val_idx in folds :        \n",
    "        # train model and compute MSE on test fold\n",
    "        pipe_lassoCV = make_pipeline(PolynomialFeatures(degree=3, include_bias=False),\n",
    "                                     StandardScaler(),\n",
    "                                     Lasso(max_iter = 10000, alpha=lambda__, tol=tol))            \n",
    "        X_train, y_train, = X_dev[train_idx], y_dev[train_idx]\n",
    "        X_val, y_val = X_dev[val_idx], y_dev[val_idx] \n",
    "        pipe_lassoCV.fit(X_train, y_train)        \n",
    "        mseCV_.append(mse(pipe_lassoCV.predict(X_val), y_val))\n",
    "        \n",
    "    # store result    \n",
    "    mseCV.append(mseCV_) \n",
    "    \n",
    "# convert to DataFrame\n",
    "lambdaCV = pd.DataFrame(mseCV, index=lambda_)\n",
    "lambdaCV['m'] = lambdaCV.mean(axis=1)\n",
    "lambda_o_ = lambdaCV['m'].nsmallest(1).index\n",
    "\n",
    "pipe_lassoCV = make_pipeline(PolynomialFeatures(degree=3, include_bias=False),\n",
    "                                     StandardScaler(),\n",
    "                                     Lasso(max_iter = 10000, alpha=lambda_o_, tol=tol))\n",
    "\n",
    "pipe_lassoCV.fit(X_dev, y_dev)\n",
    "print(lambda_o_,' ',round(mse(pipe_lassoCV.predict(X_test),y_test),3), round(np.sqrt(mse(pipe_lassoCV.predict(X_test),y_test)),3))\n",
    "\n",
    "poly = PolynomialFeatures(3, False)\n",
    "stdr = StandardScaler()\n",
    "poly.fit(X_dev)\n",
    "X_dev_ = poly.transform(X_dev)\n",
    "X_test_ = poly.transform(X_test)\n",
    "X_dev_ = stdr.fit_transform(X_dev_)\n",
    "X_test_ = stdr.transform(X_test_)\n",
    "\n",
    "\n",
    "lasso = Lasso(max_iter = 10000, alpha=lambda_o_, tol=tol)\n",
    "lasso.fit(X_dev_ , y_dev)\n",
    "round(mse(lasso.predict(X_test_), y_test),3) # is same as pipeline model above\n",
    "print(lasso.score(X_test_, y_test))\n",
    "\n",
    "\n",
    "X_columns = list(df.drop(['z_sqm_price', 'Type_ Villa ', 'Municipality_København K'], axis=1).columns)\n",
    "X_columns = poly.get_feature_names(X_columns)\n",
    "\n",
    "coefs = pd.DataFrame(lasso.coef_, index = X_columns)\n",
    "coefs = coefs[coefs[0]!= 0]\n",
    "coefs['abs'] = abs(coefs[0])\n",
    "coefs['b'] = round(coefs[0],3)\n",
    "coefs.to_csv('coefs.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer loop: lambdas\n",
    "mseCV = []\n",
    "for l1_ratio__ in l1_ratio_:    \n",
    "    # inner loop: folds\n",
    "    mseCV_ = []    \n",
    "    for train_idx, val_idx in folds :        \n",
    "        # train model and compute MSE on test fold\n",
    "        pipe_ridgeCV = make_pipeline(PolynomialFeatures(degree = 3, include_bias=True), \n",
    "                           StandardScaler(),\n",
    "                           Ridge(alpha = l1_ratio__, tol=tol))          \n",
    "        X_train, y_train, = X_dev[train_idx], y_dev[train_idx]\n",
    "        X_val, y_val = X_dev[val_idx], y_dev[val_idx] \n",
    "        pipe_ridgeCV.fit(X_train, y_train)        \n",
    "        mseCV_.append(mse(pipe_ridgeCV.predict(X_val), y_val))    \n",
    "        \n",
    "    # store result    \n",
    "    mseCV.append(mseCV_) \n",
    "    \n",
    "# convert to DataFrame\n",
    "lambdaCV = pd.DataFrame(mseCV, index=lambda_)\n",
    "lambdaCV['m'] = lambdaCV.mean(axis=1)\n",
    "lambda_o_ = lambdaCV['m'].nsmallest(1).index\n",
    "\n",
    "\n",
    "pipe_ridgeCV = make_pipeline(PolynomialFeatures(degree = 3, include_bias=True), \n",
    "                           StandardScaler(),\n",
    "                           Ridge(alpha = l1_ratio__, tol=tol))\n",
    "\n",
    "pipe_ridgeCV.fit(X_dev, y_dev)\n",
    "print(lambda_o_,' ', round(mse(pipe_ridgeCV.predict(X_test),y_test),3), round(np.sqrt(mse(pipe_ridgeCV.predict(X_test),y_test)),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_el = make_pipeline(PolynomialFeatures(include_bias=False), \n",
    "                        StandardScaler(),\n",
    "                        ElasticNet(tol=.01))\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_el, \n",
    "                  param_grid={'elasticnet__alpha':np.logspace(-1,1,22),\n",
    "                              'elasticnet__l1_ratio':np.linspace(0,3,10)}, \n",
    "                  scoring='neg_mean_squared_error', \n",
    "                  n_jobs=8,\n",
    "                  iid=False,\n",
    "                  cv=10)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "gs.predict(X_train)\n",
    "\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(lambda_o_,' ', round(mse(gs.predict(X_test),y_test),3), round(np.sqrt(mse(gs.predict(X_test),y_test)),3))\n",
    "\n",
    "\n",
    "lambda_o_ = gs.best_params_\n",
    "lambda_o_['elasticnet__alpha']\n",
    "\n",
    "pipe_el = make_pipeline(PolynomialFeatures(include_bias=False), \n",
    "                        StandardScaler(),\n",
    "                        ElasticNet(alpha = lambda_o_['elasticnet__alpha'],\n",
    "                                   l1_ratio= lambda_o_['elasticnet__l1_ratio'],\n",
    "                                   tol=.01))\n",
    "\n",
    "pipe_el.fit(X_dev, y_dev)\n",
    "pipe_el.predict(X_test)\n",
    "\n",
    "round(mse(pipe_el.predict(X_test),y_test),3), round(np.sqrt(mse(pipe_el.predict(X_test),y_test)),3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
